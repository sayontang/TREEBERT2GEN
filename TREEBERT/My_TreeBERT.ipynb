{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3610jvsc74a57bd0b3174a1ca514c82740fdb711c545ebcc890d580e90aa5ede5ddc86ad6655ae32",
   "display_name": "Python 3.6.10 64-bit ('Pytorch': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "from solver import Solver\n",
    "from utils import *\n",
    "from models import *\n",
    "from dataset import *\n",
    "from copy import deepcopy as cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = argparse.Namespace()\n",
    "args.no_cuda = True\n",
    "args.model_dir ='train_model'\n",
    "args.seq_length = 50\n",
    "args.batch_size = 3\n",
    "args.num_step = 10\n",
    "args.data_dir ='data_dir'\n",
    "args.load = False\n",
    "args.train= True\n",
    "args.test = False\n",
    "args.valid_path ='data/valid.txt'\n",
    "args.train_path ='data/train.txt'\n",
    "args.test_path ='data/test.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solver = Solver(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_yielder = solver.data_utils.train_data_yielder()\n",
    "batch = data_yielder.__next__()\n",
    "print(batch['input'].size(), batch['input_mask'].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "op = solver.model(inputs = batch['input'], mask = batch['input_mask'])"
   ]
  },
  {
   "source": [
    "- --\n",
    "- --\n",
    "- --"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config():\n",
    "    \"Configuration for BERT model\"\n",
    "    vocab_size: int = 30522 # Size of Vocabulary\n",
    "    dim: int = 768 # Dimension of Hidden Layer in Transformer Encoder\n",
    "    n_layers: int = 12 # Numher of Hidden Layers\n",
    "    n_heads: int = 12 # Numher of Heads in Multi-Headed Attention Layers\n",
    "    dim_ff: int = 768*4 # Dimension of Intermediate Layers in Positionwise Feedforward Net\n",
    "    activ_fn: str = \"gelu\" # Non-linear Activation Function Type in Hidden Layers\n",
    "    p_drop_hidden: float = 0.1 # Probability of Dropout of various Hidden Layers\n",
    "    p_drop_attn: float = 0.1 # Probability of Dropout of Attention Layers\n",
    "    max_len: int = 512 # Maximum Length for Positional Embeddings\n",
    "    n_segments: int = 2 # Number of Sentence Segments\n",
    "    layer_norm_eps: int = 1e-12 # eps value for the LayerNorms\n",
    "    output_attentions : bool = False # Weather to output the attention scores\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertEmbeddings(nn.Module):\n",
    "    \"The embedding module from word, position and token_type embeddings.\"\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.word_embeddings = nn.Embedding(cfg.vocab_size, cfg.dim, padding_idx=0) # token embedding\n",
    "        self.position_embeddings = nn.Embedding(cfg.max_len, cfg.dim) # position embedding\n",
    "        self.token_type_embeddings = nn.Embedding(cfg.n_segments, cfg.dim) # segment(token type) embedding\n",
    "\n",
    "        self.LayerNorm = nn.LayerNorm(cfg.dim, eps=cfg.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(cfg.p_drop_hidden)\n",
    "\n",
    "    def forward(self, x, seg):\n",
    "        seq_len = x.size(1)\n",
    "        pos = torch.arange(seq_len, dtype=torch.long, device=x.device)\n",
    "        pos = pos.unsqueeze(0).expand_as(x) # (S,) -> (B, S)\n",
    "\n",
    "        e = self.word_embeddings(x) + self.position_embeddings(pos) + self.token_type_embeddings(seg)\n",
    "        e = self.LayerNorm(e)\n",
    "        e = self.dropout(e)\n",
    "        return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X -> self_attn -> X [calculated by self attention]\n",
    "class BertSelfAttention(nn.Module):\n",
    "    \"\"\" Multi-Headed Dot Product Attention \"\"\"\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.query = nn.Linear(cfg.dim, cfg.dim)\n",
    "        self.key = nn.Linear(cfg.dim, cfg.dim)\n",
    "        self.value = nn.Linear(cfg.dim, cfg.dim)\n",
    "        self.dropout = nn.Dropout(cfg.p_drop_attn)\n",
    "        self.n_heads = cfg.n_heads\n",
    "\n",
    "    def forward(self, x, attention_mask = None, output_attentions = False):\n",
    "\n",
    "        B, S, D = x.shape\n",
    "        H = self.n_heads\n",
    "        W = int( D/H )\n",
    "        assert W * H == D\n",
    "\n",
    "        q, k, v = self.query(x), self.key(x), self.value(x)\n",
    "        q, k, v = q.reshape((B, S, H, W)), k.reshape((B, S, H, W)), v.reshape((B, S, H, W))\n",
    "        q, k, v = q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2)\n",
    "\n",
    "        attn_scores = q @ k.transpose(-2, -1) / np.sqrt(k.size(-1))\n",
    "        \n",
    "        if attention_mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(attention_mask[:, None, None, :] == 0, -1e9)\n",
    "        attn_scores = self.dropout(F.softmax(attn_scores, dim=-1))\n",
    "\n",
    "        hidden_states = (attn_scores @ v).transpose(1, 2).contiguous()\n",
    "        hidden_states = hidden_states.reshape(B, S, D)\n",
    "        return (hidden_states, attn_scores) if output_attentions else (hidden_states,)"
   ]
  },
  {
   "source": [
    "- $ a = \\begin{matrix}  0 & 1 & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 1 \\\\ 0 & 0 & 0 & 0 \\\\ \\end{matrix} $ &#8594\n",
    "$ b = \\begin{matrix}  1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 1 \\\\ \\end{matrix} $ &#8594 \n",
    "$ c = \\begin{matrix}  0 & 0 & 0 & 0 \\\\ 1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\\\ \\end{matrix} $ &#8594\n",
    "$ tri\\_matrix = \\begin{matrix}  1 & 1 & 1 & 1 \\\\ 0 & 1 & 1 & 1 \\\\ 0 & 0 & 1 & 1 \\\\ 0 & 0 & 0 & 1 \\\\ \\end{matrix} $\n",
    "$ (a + c) = \\begin{matrix} 0 & 1 & 0 & 0 \\\\ 1 & 0 & 1 & 0 \\\\ 0 & 1 & 0 & 1 \\\\ 0 & 0 & 1 & 0 \\\\ \\end{matrix} $\n",
    "\n",
    "- $ SCORE =  \\begin{matrix}  \n",
    "\\langle Q_0, K_0 \\rangle & \\dots & \\langle Q_0, K_3 \\rangle\\\\ \\vdots & \\ddots & \\\\ \\langle Q_3, K_0 \\rangle &  & \\langle Q_3, K_3 \\rangle \\\\ \n",
    "\\end{matrix}\n",
    "= \\begin{matrix} S_{00} & \\dots & S_{03} \\\\ \\vdots & \\ddots & \\\\S_{30} & & S_{33} \\\\ \\end{matrix} $\n",
    "\n",
    "- $ SCORE  \\xrightarrow[- \\infty]{MASK(a+c)} \n",
    "\\begin{matrix} \n",
    "-\\infty & S_{01} & -\\infty & -\\infty \\\\ S_{10} & -\\infty & S_{12} & -\\infty \\\\ -\\infty & S_{21} & -\\infty & S_{23} \\\\ -\\infty & -\\infty & S_{32} & -\\infty \\\\ \\end{matrix} \n",
    "\\xrightarrow[]{SOFTMAAX(dim = 0)} \n",
    "\\begin{matrix} \n",
    "0 & P_{01} & 0 & 0 \\\\ P_{10} & 0 & P_{12} & 0 \\\\ 0 & P_{21} & 0 & P_{23} \\\\ 0 & 0 & P_{32} & 0 \\\\ \\end{matrix}  \n",
    "\\xrightarrow[]{Score^T\\dot (Score+\\epsilon)} \n",
    "\\begin{matrix} \n",
    "0 & \\sqrt{\\langle P_{01}, P_{01} +\\epsilon \\rangle} & 0 & 0 \\\\ \n",
    "\\sqrt{\\langle P_{10},P_{01} +\\epsilon \\rangle} & 0 & \\sqrt{\\langle P_{12},P_{21} +\\epsilon \\rangle} & 0 \\\\ \n",
    "0 & \\sqrt{\\langle P_{21},P_{12} +\\epsilon \\rangle }& 0 & \\sqrt{\\langle P_{23},P_{32} +\\epsilon \\rangle} \\\\ \n",
    "0 & 0 & \\sqrt{\\langle P_{32},P_{23} +\\epsilon \\rangle} & 0 \\\\ \\end{matrix}$\n",
    "\n",
    "- $\\hat{A^l} \\sim\n",
    "\\begin{matrix} \n",
    "0 & \\sqrt{\\langle P_{01}, P_{01}\\rangle} & 0 & 0 \\\\ \n",
    "\\sqrt{\\langle P_{10}P_{01} \\rangle} & 0 & \\sqrt{\\langle P_{12}P_{21} \\rangle} & 0 \\\\ \n",
    "0 & \\sqrt{\\langle P_{21}P_{12} \\rangle }& 0 & \\sqrt{\\langle P_{23}P_{32} \\rangle} \\\\ \n",
    "0 & 0 & \\sqrt{\\langle P_{32}P_{23} \\rangle}& 0 \\\\ \\end{matrix}\n",
    "\\equiv\n",
    "\\begin{matrix} \n",
    "0 & \\hat{a_0} & 0 & 0  \\\\ \n",
    "\\hat{a_0} & 0 & \\hat{a_1} & 0 \\\\ \n",
    "0 & \\hat{a_1} & 0 & \\hat{a_2} \\\\ \n",
    "0 & 0 & \\hat{a_2} & 0 \\\\ \\end{matrix}$\n",
    "\n",
    "- $A^l = A^{l-1} + (1 - A^{l-1})\\hat{A^l} = \n",
    "\\begin{matrix} 0 & a_0 & 0 & 0  \\\\ a_0 & 0 & a_1 & 0 \\\\ 0 & a_1 & 0 & a_2 \\\\ 0 & 0 & a_2 & 0 \\\\ \\end{matrix}\n",
    "\\xrightarrow[0]{MASK(a)} \n",
    "\\begin{matrix} 0 & log(a_0 + \\epsilon) & 0 & 0  \\\\ 0 & 0 & log(a_1 + \\epsilon) & 0 \\\\ 0 & 0 & 0 & log(a_2 + \\epsilon) \\\\ 0 & 0 & 0 & 0 \\\\ \\end{matrix}\n",
    "\\xrightarrow[]{C.tri\\_matrix}\n",
    "\\begin{matrix} \n",
    "0 & log(a_0 + \\epsilon) & log(a_0+ \\epsilon) & log(a_0+ \\epsilon)  \\\\ \n",
    "0 & 0 & log(a_1+ \\epsilon) & log(a_1+ \\epsilon) \\\\ \n",
    "0 & 0 & 0 & log(a_2 + \\epsilon) \\\\ \n",
    "0 & 0 & 0 & 0 \\\\ \\end{matrix}\n",
    "\\xrightarrow[tri\\_matrix^T.C]{log()} \\sim\n",
    "\\begin{matrix} \n",
    "0 &  log(a_0 + \\epsilon) & log(a_0)+log(a_1) & log(a_0)+log(a_1)+log(a_2)  \\\\ \n",
    "0 & 0 & log(a_1) & log(a_1)+log(a_2) \\\\ \n",
    "0 & 0 & 0 & log(a_2) \\\\ \n",
    "0 & 0 & 0 & 0 \\\\ \n",
    "\\end{matrix}$\n",
    "\n",
    "- $ \\equiv \n",
    "\\begin{matrix} 0 & log(a_0) & log(a_0a_1) & log(a_0a_1a_2)  \\\\ \n",
    "0 & 0 & log(a_1) & log(a_1a_2) \\\\ \n",
    "0 & 0 & 0 & log(a_2) \\\\ \n",
    "0 & 0 & 0 & 0 \\\\ \n",
    "\\end{matrix}\n",
    "\\xrightarrow[]{exp()}\n",
    "\\begin{matrix} 0 & C_{01} & C_{02} & C_{02}  \\\\ 0 & 0 & C_{12} & C_{13} \\\\ 0 & 0 & 0 & C_{23} \\\\ 0 & 0 & 0 & 0 \\\\ \\end{matrix}\n",
    "\\xrightarrow[]{ + C^T}\n",
    "\\begin{matrix} 0 & C_{01} & C_{02} & C_{02}  \\\\ \n",
    "C_{01} & 0 & C_{12} & C_{13} \\\\ \n",
    "C_{02} & C_{12} & 0 & C_{23} \\\\ \n",
    "C_{03} & C_{13} & C_{23} & 0 \\\\ \n",
    "\\end{matrix}$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupAttention(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super(GroupAttention, self).__init__()\n",
    "        self.d_model = cfg.dim\n",
    "        self.linear_key = nn.Linear(cfg.dim, cfg.dim)\n",
    "        self.linear_query = nn.Linear(cfg.dim, cfg.dim)\n",
    "        self.LayerNorm = nn.LayerNorm(cfg.dim, eps= cfg.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(cfg.p_drop_attn)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask, prior_A):\n",
    "        B, S = hidden_states.size()[:2]\n",
    "\n",
    "        context = self.LayerNorm(hidden_states)\n",
    "\n",
    "        a = torch.from_numpy(np.diag(np.ones(S - 1, dtype=np.int32),1))\n",
    "        b = torch.from_numpy(np.diag(np.ones(S, dtype=np.int32),0))\n",
    "        c = torch.from_numpy(np.diag(np.ones(S - 1, dtype=np.int32),-1))\n",
    "        tri_matrix = torch.from_numpy(np.triu(np.ones([S, S], dtype=np.float32),0))\n",
    "\n",
    "        mask = attention_mask[:, None, :] & (a+c)[None, :, :]\n",
    "        \n",
    "        key = self.linear_key(context)\n",
    "        query = self.linear_query(context)\n",
    "        \n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / (self.d_model / 2)        \n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        A = F.softmax(scores, dim=-1)\n",
    "        A = torch.sqrt(A * A.transpose(-2,-1) + 1e-9)\n",
    "        A = prior_A + (1. - prior_A)*A\n",
    "\n",
    "        t = torch.log(A + 1e-9).masked_fill(a==0, 0).matmul(tri_matrix)\n",
    "        C_prior = tri_matrix.matmul(t).exp().masked_fill((tri_matrix.int()-b)==0, 0)    \n",
    "\n",
    "        C_prior = C_prior + C_prior.transpose(-2, -1) + torch.from_numpy(np.diag(np.ones(S)))\n",
    "        \n",
    "        return C_prior, A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tree2bert_dataset(data_path = './../Data/raw/seq2seq/train_short_prefix.txt.val',\n",
    "                            max_seq_len = 256,\n",
    "                            max_ev_len = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_tokenizer = BertWordPieceTokenizer(vocab='./vocab/vocab.txt')\n",
    "fast_tokenizer.add_special_tokens(['[et_sep]', '[ea_sep]', '[ds_sep]'])\n",
    "fast_tokenizer.enable_truncation(max_length = 256)\n",
    "fast_tokenizer.enable_padding(length=256)\n",
    "\n",
    "emb_layer = BertEmbeddings(config)\n",
    "att_layer = BertSelfAttention(config)\n",
    "grp_att_layer = GroupAttention(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_text_tok = dataset[:2]['text_tok_src'].long()\n",
    "ip_text_mask = dataset[:2]['text_mask_src'].long()\n",
    "ip_event_loc = dataset[:2]['event_loc_src'].long()\n",
    "ip_event_mask = dataset[:2]['event_mask_src'].long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_text_A = torch.ones_like(ip_text_tok) # B x S\n",
    "\n",
    "# Create diagonal for the A matrix\n",
    "# all the [et_sep] and [PAD]s will have value of ~0 and rest will have value of ~1\n",
    "\n",
    "# Create the vector for the diagonal elements of the A matrix \n",
    "# # all the [et_sep] and [PAD]s will have value of ~0 and rest will have value of ~1\n",
    "#  B x S\n",
    "A_initial = cc(ip_text_mask)*0.999 \n",
    "\n",
    "# Set all the locations with [et_sep] tokens as 0.0\n",
    "A_initial = A_initial.scatter(1, ip_event_loc.long(), torch.zeros_like(ip_event_loc).float())\n",
    "\n",
    "# exclude the last token as this vector will be a diagonal with offset 1\n",
    "A_initial = A_initial[:, :-1]\n",
    "\n",
    "# Create a diagonal matrix with this A as the diagonal at offset 1\n",
    "A_initial= [torch.diag(A_initial[i, :].float(), 1)[None, :, :] for i in range(A_initial.shape[0])]\n",
    "A_initial = torch.cat(A_initial)\n",
    "\n",
    "# Add it's transpose, as the A matrix is supposed to be symmetric\n",
    "A_initial += A_initial.transpose(-1, -2).contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.9990, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.9990, 0.0000, 0.9990, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.9990, 0.0000, 0.9990, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.9990, 0.0000, 0.9990, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.9990, 0.0000, 0.9990],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.9990, 0.0000]])"
      ]
     },
     "metadata": {},
     "execution_count": 330
    }
   ],
   "source": [
    "A_initial[0, :6, :6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = emb_layer(x = ip_text_tok.long(), seg = torch.zeros_like(ip_text_tok).long())\n",
    "hidden_event_subset = hidden[np.arange(hidden.shape[0])[:, None], ip_event_loc.long()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = config.dim\n",
    "linear_key = nn.Linear(config.dim, config.dim)\n",
    "linear_query = nn.Linear(config.dim, config.dim)\n",
    "LayerNorm = nn.LayerNorm(config.dim, eps= config.layer_norm_eps)\n",
    "dropout = nn.Dropout(config.p_drop_attn)\n",
    "\n",
    "B, S = hidden_event_subset.size()[:2]\n",
    "\n",
    "context = LayerNorm(hidden_event_subset)\n",
    "\n",
    "a = torch.from_numpy(np.diag(np.ones(S - 1, dtype=np.int32),1))\n",
    "b = torch.from_numpy(np.diag(np.ones(S, dtype=np.int32),0))\n",
    "c = torch.from_numpy(np.diag(np.ones(S - 1, dtype=np.int32),-1))\n",
    "tri_matrix = torch.from_numpy(np.triu(np.ones([S, S], dtype=np.float32),0))\n",
    "\n",
    "a_full = torch.from_numpy(np.diag(np.ones(256 - 1, dtype=np.int32),1))\n",
    "b_full = torch.from_numpy(np.diag(np.ones(256, dtype=np.int32),0))\n",
    "c_full = torch.from_numpy(np.diag(np.ones(256 - 1, dtype=np.int32),-1))\n",
    "tri_matrix_full = torch.from_numpy(np.triu(np.ones([256, 256], dtype=np.float32),0))\n",
    "\n",
    "mask = ip_event_mask[:, None, :] & (a+c)[None, :, :]\n",
    "\n",
    "key = linear_key(context)\n",
    "query = linear_query(context)\n",
    "\n",
    "scores = torch.matmul(query, key.transpose(-2, -1)) / (d_model / 2)        \n",
    "scores = scores.masked_fill(mask == 0, -1e10)\n",
    "\n",
    "A = F.softmax(scores, dim=-1)\n",
    "A = torch.sqrt(A * A.transpose(-2,-1) + 1e-10)\n",
    "A = torch.cat([torch.diagonal(A[batch], 1)[None, :] for batch in range(B)])\n",
    "\n",
    "A_new = cc(ip_text_mask).float()\n",
    "A_new = A_new.scatter(1, ip_event_loc[:, :-1].long(), A.float())\n",
    "\n",
    "A_new = A_new[:, :-1]\n",
    "A_new= [torch.diag(A_new[b, :], 1)[None, :, :] for b in range(A_new.shape[0])]\n",
    "A_new = torch.cat(A_new)\n",
    "A_new += A_new.transpose(-1, -2).contiguous()\n",
    "A_new = A_initial + (1. - A_initial)*A_new\n",
    "\n",
    "t = torch.log(A_new + 1e-10).masked_fill(a_full==0, 0).matmul(tri_matrix_full)\n",
    "C_prior = tri_matrix_full.matmul(t).exp().masked_fill((tri_matrix_full.int()-b_full)==0, 0)    \n",
    "C_prior = C_prior + C_prior.transpose(-2, -1) + torch.from_numpy(np.diag(np.ones(256)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 0.7112], dtype=torch.float64, grad_fn=<SliceBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 359
    }
   ],
   "source": [
    "C_prior[0, 0, :12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[ 10,  30,  39,  59,  68,  94, 104, 106, 107, 108, 109, 110, 111, 112,\n",
       "         113, 114, 115, 116, 117, 118],\n",
       "        [ 13,  31,  44,  59,  70,  85,  87,  88,  89,  90,  91,  92,  93,  94,\n",
       "          95,  96,  97,  98,  99, 100]])"
      ]
     },
     "metadata": {},
     "execution_count": 336
    }
   ],
   "source": [
    "ip_event_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'[CLS] some devices [ea_sep] can keep more efficiently [ea_sep] boots [et_sep]'"
      ]
     },
     "metadata": {},
     "execution_count": 353
    }
   ],
   "source": [
    "fast_tokenizer.decode(ip_text_tok.numpy()[0, :11], skip_special_tokens = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m\n",
      "\u001b[0mfast_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mids\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Decode the given list of ids to a string sequence\n",
      "\n",
      "Args:\n",
      "    ids: List[unsigned int]:\n",
      "        A list of ids to be decoded\n",
      "\n",
      "    skip_special_tokens: (`optional`) boolean:\n",
      "        Whether to remove all the special tokens from the output string\n",
      "\n",
      "Returns:\n",
      "    The decoded string\n",
      "\u001b[0;31mFile:\u001b[0m      ~/miniconda3/envs/Pytorch/lib/python3.6/site-packages/tokenizers/implementations/base_tokenizer.py\n",
      "\u001b[0;31mType:\u001b[0m      method\n"
     ],
     "name": "stdout"
    }
   ],
   "source": [
    "fast_tokenizer.decode?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}