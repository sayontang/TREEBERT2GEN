{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3610jvsc74a57bd0b3174a1ca514c82740fdb711c545ebcc890d580e90aa5ede5ddc86ad6655ae32",
   "display_name": "Python 3.6.10 64-bit ('Pytorch': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "### **Structure of BERT**\n",
    "\n",
    "```\n",
    "BertEmbeddings(\n",
    "  (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
    "  (position_embeddings): Embedding(512, 768)\n",
    "  (token_type_embeddings): Embedding(2, 768)\n",
    "  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "  (dropout): Dropout(p=0.1, inplace=False)\n",
    ")\n",
    "```\n",
    "\n",
    "```\n",
    "BertLayer(\n",
    "  (attention): BertAttention(\n",
    "    (self): BertSelfAttention(\n",
    "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
    "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
    "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
    "      (dropout): Dropout(p=0.1, inplace=False)\n",
    "    )\n",
    "    (output): BertSelfOutput(\n",
    "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
    "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "      (dropout): Dropout(p=0.1, inplace=False)\n",
    "    )\n",
    "  )\n",
    "  (intermediate): BertIntermediate(\n",
    "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
    "  )\n",
    "  (output): BertOutput(\n",
    "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
    "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "    (dropout): Dropout(p=0.1, inplace=False)\n",
    "  )\n",
    ")\n",
    "```\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import subprocess\n",
    "from models import *\n",
    "from utils import *\n",
    "from parse import *\n",
    "import random\n",
    "from bert_optimizer import BertAdam\n",
    "from copy import deepcopy as cc\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import gc\n",
    "from torch import nn\n",
    "from torch.nn import GELU as GELU\n",
    "gc.disable()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config():\n",
    "    \"Configuration for BERT model\"\n",
    "    vocab_size: int = 30522 # Size of Vocabulary\n",
    "    dim: int = 768 # Dimension of Hidden Layer in Transformer Encoder\n",
    "    n_layers: int = 12 # Numher of Hidden Layers\n",
    "    n_heads: int = 12 # Numher of Heads in Multi-Headed Attention Layers\n",
    "    dim_ff: int = 768*4 # Dimension of Intermediate Layers in Positionwise Feedforward Net\n",
    "    activ_fn: str = \"gelu\" # Non-linear Activation Function Type in Hidden Layers\n",
    "    p_drop_hidden: float = 0.1 # Probability of Dropout of various Hidden Layers\n",
    "    p_drop_attn: float = 0.1 # Probability of Dropout of Attention Layers\n",
    "    max_len: int = 512 # Maximum Length for Positional Embeddings\n",
    "    n_segments: int = 2 # Number of Sentence Segments\n",
    "    layer_norm_eps: int = 1e-12 # eps value for the LayerNorms\n",
    "    output_attentions : bool = False # Weather to output the attention scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the tree-transformer\n",
    "\n",
    "# h = 8\n",
    "# d_model = 768\n",
    "# cuda_present = True\n",
    "# d_ff = 2048\n",
    "# dropout = 0.1\n",
    "# vocab_size = 30522\n",
    "# N = 10\n",
    "# attn = MultiHeadedAttention(h, d_model, no_cuda = True)\n",
    "# group_attn = GroupAttention(d_model, no_cuda = False)\n",
    "# ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "# position = PositionalEncoding(d_model, dropout)\n",
    "# word_embed = nn.Sequential(Embeddings(d_model, vocab_size), cc(position))\n",
    "# model = Encoder(EncoderLayer(d_model, cc(attn), cc(ff), group_attn, dropout), \n",
    "#         N, d_model, vocab_size, cc(word_embed))\n",
    "cnfg = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the BERT\n",
    "BERT = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "source": [
    "### Embedding Layer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertEmbeddings(nn.Module):\n",
    "    \"The embedding module from word, position and token_type embeddings.\"\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.word_embeddings = nn.Embedding(cfg.vocab_size, cfg.dim, padding_idx=0) # token embedding\n",
    "        self.position_embeddings = nn.Embedding(cfg.max_len, cfg.dim) # position embedding\n",
    "        self.token_type_embeddings = nn.Embedding(cfg.n_segments, cfg.dim) # segment(token type) embedding\n",
    "\n",
    "        self.LayerNorm = nn.LayerNorm(cfg.dim, eps=cfg.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(cfg.p_drop_hidden)\n",
    "\n",
    "    def forward(self, x, seg):\n",
    "        seq_len = x.size(1)\n",
    "        pos = torch.arange(seq_len, dtype=torch.long, device=x.device)\n",
    "        pos = pos.unsqueeze(0).expand_as(x) # (S,) -> (B, S)\n",
    "\n",
    "        e = self.word_embeddings(x) + self.position_embeddings(pos) + self.token_type_embeddings(seg)\n",
    "        e = self.LayerNorm(e)\n",
    "        e = self.dropout(e)\n",
    "        return e"
   ]
  },
  {
   "source": [
    "### Self Attention Layer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# X -> self_attn -> X [calculated by self attention]\n",
    "class BertSelfAttention(nn.Module):\n",
    "    \"\"\" Multi-Headed Dot Product Attention \"\"\"\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.query = nn.Linear(cfg.dim, cfg.dim)\n",
    "        self.key = nn.Linear(cfg.dim, cfg.dim)\n",
    "        self.value = nn.Linear(cfg.dim, cfg.dim)\n",
    "        self.dropout = nn.Dropout(cfg.p_drop_attn)\n",
    "        self.n_heads = cfg.n_heads\n",
    "\n",
    "    def forward(self, x, mask = None, output_attentions = False):\n",
    "        \"\"\"\n",
    "        x, q(query), k(key), v(value) : (B(batch_size), S(seq_len), D(dim))\n",
    "        mask : (B(batch_size) x S(seq_len))\n",
    "        * split D(dim) into (H(n_heads), W(width of head)) ; D = H * W\n",
    "        \"\"\"\n",
    "        B, S, D = x.shape\n",
    "        H = self.n_heads\n",
    "        W = int( D/H )\n",
    "        assert W * H == D\n",
    "\n",
    "        # (B, S, D) -proj-> (B, S, D) -split-> (B, S, H, W) -trans-> (B, H, S, W)\n",
    "        q, k, v = self.query(x), self.key(x), self.value(x)\n",
    "        q, k, v = q.reshape((B, S, H, W)), k.reshape((B, S, H, W)), v.reshape((B, S, H, W))\n",
    "        q, k, v = q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2)\n",
    "\n",
    "        # (B, H, S, W) @ (B, H, W, S) -> (B, H, S, S) -Masking -> softmax-> (B, H, S, S)\n",
    "        scores = q @ k.transpose(-2, -1) / np.sqrt(k.size(-1))\n",
    "        \n",
    "        # Set the attention score at places where MASK = 0 to very low value (-1e9)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        scores = self.dropout(F.softmax(scores, dim=-1))\n",
    "        \n",
    "        # (B, H, S, S) @ (B, H, S, W) -> (B, H, S, W) -trans-> (B, S, H, W) -merge-> (B, S, D)\n",
    "        hidden_states = (scores @ v).transpose(1, 2).contiguous()\n",
    "        hidden_states = hidden_states.reshape(B, S, D)\n",
    "        return (hidden_states, scores) if output_attentions else (hidden_states,)\n",
    "\n",
    "\n",
    "#  f(X) -> Linear (D => D) -> Dropout -> X1\n",
    "#  X, f(X) -> LayerNorm( X + X1 )\n",
    "class BertSelfOutput(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(cfg.dim, cfg.dim)\n",
    "        self.LayerNorm = nn.LayerNorm(cfg.dim, eps= cfg.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(cfg.p_drop_hidden)\n",
    "\n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "# X -> self_attn -> f(X) -> Linear (D => D) -> Dropout -> X1\n",
    "# Y = LayerNorm( X + X1 )\n",
    "class BertAttention(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.self = BertSelfAttention(cfg)\n",
    "        self.output = BertSelfOutput(cfg)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None, output_attentions = False):\n",
    "        self_outputs = self.self(hidden_states,  attention_mask, output_attentions)\n",
    "        attention_output = self.output(self_outputs[0], hidden_states)\n",
    "\n",
    "        # add attentions if we output them; self_outputs[1:] is the attn score, attention_output is the hidden representation\n",
    "        outputs = (attention_output,) + self_outputs[1:] if output_attentions else (attention_output,)\n",
    "        return outputs        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_ip = torch.randint(low = 0, high = 100, size=(3, 10))\n",
    "\n",
    "bert_emb = BertEmbeddings(cnfg)\n",
    "bert_self_attn = BertAttention(cnfg)\n",
    "\n",
    "op = bert_emb(sample_ip, torch.zeros_like(sample_ip))\n",
    "op = bert_self_attn(hidden_states = op, output_attentions = False)"
   ]
  },
  {
   "source": [
    "### Forward Expansion layer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f(X) -> Linear(D => 4xD) -> GELU() -> X2\n",
    "class BertIntermediate(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(cfg.dim, cfg.dim_ff)\n",
    "        self.GELU = nn.GELU()\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.GELU(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "# f(X) -> Linear(4xD => D) -> Drouput -> X1\n",
    "#  X, f(X) -> LayerNorm( X + X1 )\n",
    "\n",
    "class BertOutput(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(cfg.dim_ff, cfg.dim)\n",
    "        self.LayerNorm = nn.LayerNorm(cfg.dim, eps=cfg.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(cfg.p_drop_hidden)\n",
    "\n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "        return hidden_states\n"
   ]
  },
  {
   "source": [
    "### Defining single BERT-encoder layer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "25772"
      ]
     },
     "metadata": {},
     "execution_count": 147
    }
   ],
   "source": [
    "bert_emb = BertEmbeddings(cnfg)\n",
    "bert_self_attn = BertAttention(cnfg)\n",
    "Bert_intermeidate = BertIntermediate(cnfg)\n",
    "Bert_op = BertOutput(cnfg)\n",
    "\n",
    "\n",
    "sample_ip = torch.randint(low = 0, high = 100, size=(3, 10))\n",
    "op = bert_emb(sample_ip, torch.zeros_like(sample_ip))\n",
    "op_attn = bert_self_attn(hidden_states = op, output_attentions = True)\n",
    "op = Bert_intermeidate(hidden_states = op_attn[0])\n",
    "op = Bert_op(hidden_states = op, input_tensor = op_attn[0])\n",
    "op.shape\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertLayer(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.attention = BertAttention(cfg)\n",
    "        self.intermediate = BertIntermediate(cfg)\n",
    "        self.output = BertOutput(cfg)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None, output_attentions = False):\n",
    "        self_attention_outputs = self.attention(hidden_states, attention_mask, output_attentions)\n",
    "        attention_output = self_attention_outputs[0]\n",
    "        outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n",
    "\n",
    "        intermediate_output = self.intermediate(attention_output)\n",
    "        layer_output = self.output(intermediate_output, attention_output)\n",
    "        outputs = (layer_output,) + outputs if output_attentions else (layer_output,)\n",
    "        return outputs"
   ]
  },
  {
   "source": [
    "### Defining the BERT model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertEncoder(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layer = nn.ModuleList([BertLayer(cfg) for _ in range(cfg.n_layers)])\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None, output_attentions = False, output_hidden_states = False):        \n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "        all_self_attentions = () if output_attentions else None\n",
    "\n",
    "        for i, layer_module in enumerate(self.layer):\n",
    "            print(f'>> Layer {i}')\n",
    "            if output_hidden_states:\n",
    "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "            layer_outputs = layer_module(hidden_states, attention_mask, output_attentions)\n",
    "            hidden_states = layer_outputs[0]\n",
    "\n",
    "            if output_attentions:\n",
    "                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n",
    "\n",
    "        outputs = (hidden_states,) \n",
    "        outputs = outputs + (all_hidden_states,) + (all_self_attentions,)\n",
    "        return outputs  # last-layer hidden state, (all hidden states), (all attentions)\n",
    "\n",
    "\n",
    "class my_BERT(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.embeddings = BertEmbeddings(cfg)\n",
    "        self.encoder = BertEncoder(cfg)\n",
    "    \n",
    "    def forward(self, x, seg, attention_mask=None, output_attentions = False, output_hidden_states = False):\n",
    "\n",
    "        embedding_output = self.embeddings(x, seg)\n",
    "        encoded_output = self.encoder(hidden_states = embedding_output, \n",
    "                                      attention_mask = attention_mask, \n",
    "                                      output_attentions = output_attentions, \n",
    "                                      output_hidden_states = output_hidden_states)\n",
    "\n",
    "        return encoded_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()\n",
    "MY_BERT = my_BERT(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_BERT_params = set([n for n, _ in MY_BERT.named_parameters()])\n",
    "BERT_params = set([n for n, _ in BERT.named_parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "197"
      ]
     },
     "metadata": {},
     "execution_count": 183
    }
   ],
   "source": [
    "len(my_BERT_params.intersection(BERT_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ">> Layer 0\n>> Layer 1\n>> Layer 2\n>> Layer 3\n>> Layer 4\n>> Layer 5\n>> Layer 6\n>> Layer 7\n>> Layer 8\n>> Layer 9\n>> Layer 10\n>> Layer 11\n"
     ]
    }
   ],
   "source": [
    "temp = torch.randint(low = 0, high = 100, size = (4, 20))\n",
    "encoded_temp = MY_BERT(x = temp, seg = torch.zeros_like(temp),  output_hidden_states = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(1, 2, 3)"
      ]
     },
     "metadata": {},
     "execution_count": 154
    }
   ],
   "source": [
    "a = (1,)\n",
    "a + (2,) + (3,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[[-1.1848, -0.0285,  0.5436,  ..., -1.1377, -0.8266, -0.7126],\n",
       "         [-0.0309, -0.4084, -0.8200,  ..., -0.6317, -0.9142,  0.2376],\n",
       "         [-0.6651,  0.5134,  0.2074,  ..., -1.2302, -0.4554, -0.4232],\n",
       "         ...,\n",
       "         [ 0.4243, -0.3227,  0.7576,  ..., -0.7486, -0.8489,  1.0825],\n",
       "         [ 0.5961, -1.0654, -0.2513,  ..., -0.9364,  0.6991, -0.7306],\n",
       "         [ 0.6256,  0.7886,  0.6593,  ..., -1.5783, -0.8665,  0.0558]],\n",
       "\n",
       "        [[-0.4661, -0.6714,  1.1090,  ..., -0.6477, -1.1112, -0.9664],\n",
       "         [-0.7594,  0.1596,  0.3844,  ..., -0.9336, -1.4286,  1.4158],\n",
       "         [-0.5134, -1.2912,  0.9980,  ..., -0.5096,  0.2018,  0.3360],\n",
       "         ...,\n",
       "         [ 0.8344,  1.5394,  1.4684,  ..., -0.9579, -1.7836,  0.9458],\n",
       "         [-0.2102, -1.2844, -0.0779,  ..., -0.5074, -0.6555, -0.3627],\n",
       "         [-0.0123,  0.3298,  1.0333,  ..., -1.3660, -2.0198, -0.5635]],\n",
       "\n",
       "        [[ 0.0591, -0.3759,  1.0905,  ..., -1.9186, -0.8224, -0.3560],\n",
       "         [ 1.1063, -0.4229, -1.0585,  ..., -0.3068, -1.4879,  0.6283],\n",
       "         [-1.0665, -0.3907, -0.6662,  ..., -1.4434, -1.3181, -1.4113],\n",
       "         ...,\n",
       "         [ 0.1138,  0.3404, -0.2606,  ..., -0.2684, -1.4363,  0.0595],\n",
       "         [ 0.5994, -0.5947,  0.8254,  ..., -0.4619, -0.0117,  0.7984],\n",
       "         [ 0.1658,  0.3261,  0.6534,  ..., -1.7374, -2.3875, -0.0358]],\n",
       "\n",
       "        [[ 0.6100, -1.0533,  0.6597,  ..., -0.1555, -2.2720,  0.5268],\n",
       "         [-0.5047, -1.3035,  0.1224,  ..., -0.1852, -1.4782, -0.6153],\n",
       "         [-0.1803, -0.2848, -0.5462,  ..., -1.9155, -1.0819, -0.0102],\n",
       "         ...,\n",
       "         [ 0.6911,  0.3518,  1.0608,  ..., -0.8180, -1.1491,  0.3122],\n",
       "         [ 0.3653, -1.6210,  0.4897,  ..., -1.3958, -0.6249, -0.1208],\n",
       "         [ 1.0220, -0.8892,  0.8586,  ..., -1.0841, -2.0125, -0.5508]]],\n",
       "       grad_fn=<NativeLayerNormBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 173
    }
   ],
   "source": [
    "encoded_temp[1][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[[-1.3521, -0.0554,  0.6890,  ..., -0.9465, -0.6662, -0.9000],\n",
       "         [-0.3505, -0.3804, -0.7439,  ..., -0.2845, -0.9817, -0.2180],\n",
       "         [-0.5933,  0.4646,  0.1405,  ..., -0.8407, -0.3823, -0.5760],\n",
       "         ...,\n",
       "         [ 0.2114, -0.4964,  0.8479,  ..., -0.4648, -0.8789,  0.7657],\n",
       "         [ 0.2352, -1.0495, -0.1933,  ..., -0.6638,  0.6567, -0.8506],\n",
       "         [ 0.7089,  0.6450,  0.7922,  ..., -1.2498, -0.9352, -0.1806]],\n",
       "\n",
       "        [[-0.5991, -0.3532,  1.2220,  ..., -0.8466, -1.2140, -1.0271],\n",
       "         [-0.9246,  0.1129,  0.7574,  ..., -0.7914, -1.4160,  1.1726],\n",
       "         [-0.3398, -0.7966,  0.8751,  ..., -0.2994, -0.1088,  0.2824],\n",
       "         ...,\n",
       "         [ 0.6497,  1.7207,  1.7003,  ..., -0.6179, -1.8581,  0.8488],\n",
       "         [-0.3679, -0.9265, -0.0716,  ..., -0.2223, -0.7588, -0.3392],\n",
       "         [-0.3000,  0.5441,  1.2114,  ..., -1.1746, -1.9180, -0.8477]],\n",
       "\n",
       "        [[-0.1487, -0.5375,  1.4424,  ..., -1.6875, -0.6888, -0.7684],\n",
       "         [ 0.5754, -0.3451, -0.7638,  ..., -0.2180, -1.6630,  0.4096],\n",
       "         [-1.1882, -0.5598, -0.5986,  ..., -1.1059, -1.3621, -1.6504],\n",
       "         ...,\n",
       "         [ 0.1594,  0.1895,  0.0955,  ..., -0.2086, -1.4787, -0.1029],\n",
       "         [ 0.5009, -0.7691,  0.7147,  ..., -0.3415, -0.1470,  0.5679],\n",
       "         [ 0.0254,  0.1492,  0.7741,  ..., -1.5856, -2.5322, -0.4445]],\n",
       "\n",
       "        [[ 0.2089, -1.0470,  0.9466,  ...,  0.1545, -2.3827,  0.5235],\n",
       "         [-0.6303, -1.2271,  0.5534,  ..., -0.1940, -1.4608, -0.8506],\n",
       "         [-0.3424, -0.3471, -0.4237,  ..., -1.7640, -1.1116, -0.0113],\n",
       "         ...,\n",
       "         [ 0.2345,  0.3063,  1.1406,  ..., -0.7122, -1.1483,  0.1736],\n",
       "         [ 0.3283, -1.4974,  0.9069,  ..., -1.0159, -0.6718, -0.1981],\n",
       "         [ 0.8890, -0.9525,  0.8213,  ..., -0.7249, -1.8403, -0.5706]]],\n",
       "       grad_fn=<NativeLayerNormBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 174
    }
   ],
   "source": [
    "encoded_temp[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}