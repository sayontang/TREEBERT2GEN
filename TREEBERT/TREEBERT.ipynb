{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3610jvsc74a57bd0b3174a1ca514c82740fdb711c545ebcc890d580e90aa5ede5ddc86ad6655ae32",
   "display_name": "Python 3.6.10 64-bit ('Pytorch': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import subprocess\n",
    "from models import *\n",
    "from utils import *\n",
    "from parse import *\n",
    "import random\n",
    "from copy import deepcopy as cc\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import gc\n",
    "from torch import nn\n",
    "from torch.nn import GELU as GELU\n",
    "import argparse\n",
    "from solver import Solver\n",
    "gc.disable()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config():\n",
    "    \"Configuration for BERT model\"\n",
    "    vocab_size: int = 30522 # Size of Vocabulary\n",
    "    dim: int = 768 # Dimension of Hidden Layer in Transformer Encoder\n",
    "    n_layers: int = 12 # Numher of Hidden Layers\n",
    "    n_heads: int = 12 # Numher of Heads in Multi-Headed Attention Layers\n",
    "    dim_ff: int = 768*4 # Dimension of Intermediate Layers in Positionwise Feedforward Net\n",
    "    activ_fn: str = \"gelu\" # Non-linear Activation Function Type in Hidden Layers\n",
    "    p_drop_hidden: float = 0.1 # Probability of Dropout of various Hidden Layers\n",
    "    p_drop_attn: float = 0.1 # Probability of Dropout of Attention Layers\n",
    "    max_len: int = 512 # Maximum Length for Positional Embeddings\n",
    "    n_segments: int = 2 # Number of Sentence Segments\n",
    "    layer_norm_eps: int = 1e-12 # eps value for the LayerNorms\n",
    "    output_attentions : bool = False # Weather to output the attention scores\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "source": [
    "+ #  <u>Embedding Layer</u>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertEmbeddings(nn.Module):\n",
    "    \"The embedding module from word, position and token_type embeddings.\"\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.word_embeddings = nn.Embedding(cfg.vocab_size, cfg.dim, padding_idx=0) # token embedding\n",
    "        self.position_embeddings = nn.Embedding(cfg.max_len, cfg.dim) # position embedding\n",
    "        self.token_type_embeddings = nn.Embedding(cfg.n_segments, cfg.dim) # segment(token type) embedding\n",
    "\n",
    "        self.LayerNorm = nn.LayerNorm(cfg.dim, eps=cfg.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(cfg.p_drop_hidden)\n",
    "\n",
    "    def forward(self, x, seg):\n",
    "        seq_len = x.size(1)\n",
    "        pos = torch.arange(seq_len, dtype=torch.long, device=x.device)\n",
    "        pos = pos.unsqueeze(0).expand_as(x) # (S,) -> (B, S)\n",
    "\n",
    "        e = self.word_embeddings(x) + self.position_embeddings(pos) + self.token_type_embeddings(seg)\n",
    "        e = self.LayerNorm(e)\n",
    "        e = self.dropout(e)\n",
    "        return e"
   ]
  },
  {
   "source": [
    "* # <u>Self Attention Layer</u>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# X -> self_attn -> X [calculated by self attention]\n",
    "class BertSelfAttention(nn.Module):\n",
    "    \"\"\" Multi-Headed Dot Product Attention \"\"\"\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.query = nn.Linear(cfg.dim, cfg.dim)\n",
    "        self.key = nn.Linear(cfg.dim, cfg.dim)\n",
    "        self.value = nn.Linear(cfg.dim, cfg.dim)\n",
    "        self.dropout = nn.Dropout(cfg.p_drop_attn)\n",
    "        self.n_heads = cfg.n_heads\n",
    "\n",
    "    def forward(self, x, attention_mask = None, output_attentions = False):\n",
    "        \"\"\"\n",
    "        x, q(query), k(key), v(value) : (B(batch_size), S(seq_len), D(dim))\n",
    "        mask : (B(batch_size) x S(seq_len))\n",
    "        * split D(dim) into (H(n_heads), W(width of head)) ; D = H * W\n",
    "        attention_mask_dim = B x S (dim S is for every value in the \"x\"), so need to repeat it for every query\n",
    "        \"\"\"\n",
    "        B, S, D = x.shape\n",
    "        H = self.n_heads\n",
    "        W = int( D/H )\n",
    "        assert W * H == D\n",
    "\n",
    "        # (B, S, D) -proj-> (B, S, D) -split-> (B, S, H, W) -trans-> (B, H, S, W)\n",
    "        q, k, v = self.query(x), self.key(x), self.value(x)\n",
    "        q, k, v = q.reshape((B, S, H, W)), k.reshape((B, S, H, W)), v.reshape((B, S, H, W))\n",
    "        q, k, v = q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2)\n",
    "\n",
    "        # (B, H, S(Q), W) @ (B, H, W, S(K/V)) -> (B, H, S(Q), S(K/V)) -Masking -> softmax-> (B, H, S(Q), S(K/V))\n",
    "        attn_scores = q @ k.transpose(-2, -1) / np.sqrt(k.size(-1))\n",
    "        \n",
    "        # Set the attention score at places where MASK = 0 to very low value (-1e9)\n",
    "        if attention_mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(attention_mask[:, None, None, :] == 0, -1e9)\n",
    "        attn_scores = self.dropout(F.softmax(attn_scores, dim=-1))\n",
    "\n",
    "        # (B, H, S, S) @ (B, H, S, W) -> (B, H, S, W) -trans-> (B, S, H, W) -merge-> (B, S, D)\n",
    "        hidden_states = (attn_scores @ v).transpose(1, 2).contiguous()\n",
    "        hidden_states = hidden_states.reshape(B, S, D)\n",
    "\n",
    "        result = {}\n",
    "        result['hidden_states'] = hidden_states \n",
    "        result['attn_scores'] =  None\n",
    "        if output_attentions :\n",
    "            result['attn_scores'] =  attn_scores\n",
    "\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  f(X) -> Linear (D => D) -> Dropout -> X1\n",
    "#  X, f(X) -> LayerNorm( X + X1 )\n",
    "class BertSelfOutput(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(cfg.dim, cfg.dim)\n",
    "        self.LayerNorm = nn.LayerNorm(cfg.dim, eps= cfg.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(cfg.p_drop_hidden)\n",
    "\n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X -> self_attn -> f(X) -> Linear (D => D) -> Dropout -> X1\n",
    "# Y = LayerNorm( X + X1 )\n",
    "class BertAttention(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.self = BertSelfAttention(cfg)\n",
    "        self.output = BertSelfOutput(cfg)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None, output_attentions = False):\n",
    "        self_outputs = self.self(hidden_states,  attention_mask, output_attentions)\n",
    "        attention_output = self.output(self_outputs['hidden_states'], hidden_states)\n",
    "\n",
    "        outputs = {'hidden_states': attention_output, 'attn_scores': self_outputs['attn_scores']}\n",
    "        return outputs        "
   ]
  },
  {
   "source": [
    "* # <u>Forward Expansion layer</u>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f(X) -> Linear(D => 4xD) -> GELU() -> X2\n",
    "class BertIntermediate(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(cfg.dim, cfg.dim_ff)\n",
    "        self.GELU = nn.GELU()\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.GELU(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "# f(X) -> Linear(4xD => D) -> Drouput -> X1\n",
    "#  X, f(X) -> LayerNorm( X + X1 )\n",
    "class BertOutput(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(cfg.dim_ff, cfg.dim)\n",
    "        self.LayerNorm = nn.LayerNorm(cfg.dim, eps=cfg.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(cfg.p_drop_hidden)\n",
    "\n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "        return hidden_states"
   ]
  },
  {
   "source": [
    "* # <u>Defining single BERT-encoder layer</u>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertLayer(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.attention = BertAttention(cfg)\n",
    "        self.intermediate = BertIntermediate(cfg)\n",
    "        self.output = BertOutput(cfg)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None, output_attentions = False):\n",
    "        self_attention_outputs = self.attention(hidden_states, attention_mask, output_attentions)\n",
    "\n",
    "        intermediate_output = self.intermediate(self_attention_outputs['hidden_states'])\n",
    "        layer_output = self.output(intermediate_output, self_attention_outputs['hidden_states'])\n",
    "\n",
    "        outputs = {'hidden_states':layer_output, 'attn_scores': self_attention_outputs['attn_scores']}\n",
    "        return outputs"
   ]
  },
  {
   "source": [
    "* # <u>Defining the BERT model</u>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertEncoder(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layer = nn.ModuleList([BertLayer(cfg) for _ in range(cfg.n_layers)])\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None, output_attentions = False, output_hidden_states = False):        \n",
    "        all_hidden_states = [] if output_hidden_states else None\n",
    "        all_self_attentions = [] if output_attentions else None\n",
    "\n",
    "        for i, layer_module in enumerate(self.layer):\n",
    "            if output_hidden_states:\n",
    "                all_hidden_states.append(hidden_states)\n",
    "\n",
    "            layer_outputs = layer_module(hidden_states, attention_mask, output_attentions)\n",
    "            hidden_states, attn_scores = layer_outputs['hidden_states'], layer_outputs['attn_scores']\n",
    "\n",
    "            if output_attentions:\n",
    "                all_self_attentions.append(attn_scores)\n",
    "\n",
    "        outputs = {'hidden_states': hidden_states, 'attn_scores': all_self_attentions, 'all_hidden_states': all_hidden_states}\n",
    "        return outputs \n",
    "\n",
    "\n",
    "class my_BERT(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.embeddings = BertEmbeddings(cfg)\n",
    "        self.encoder = BertEncoder(cfg)\n",
    "    \n",
    "    def forward(self, x, seg, attention_mask=None, output_attentions = False, output_hidden_states = False):\n",
    "\n",
    "        embedding_output = self.embeddings(x, seg)\n",
    "        encoded_output = self.encoder(hidden_states = embedding_output, \n",
    "                                      attention_mask = attention_mask, \n",
    "                                      output_attentions = output_attentions, \n",
    "                                      output_hidden_states = output_hidden_states)\n",
    "\n",
    "        return encoded_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()\n",
    "MY_BERT = my_BERT(config)\n",
    "BERT = BertModel.from_pretrained('bert-base-uncased')\n",
    "MY_BERT.load_state_dict(BERT.state_dict(), strict = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Parametes in the model that are not in BERT:: {'pooler.dense.weight', 'pooler.dense.bias'}\n"
     ]
    }
   ],
   "source": [
    "MY_BERT_params = set([n for n, _ in MY_BERT.named_parameters()])\n",
    "BERT_params = set([n for n, _ in BERT.named_parameters()])\n",
    "print(f'Parametes in the model that are not in BERT:: {BERT_params.difference(MY_BERT_params)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = torch.randint(low = 0, high = 100, size = (4, 20))\n",
    "encoded_temp = MY_BERT(x = temp, seg = torch.zeros_like(temp),  attention_mask = torch.ones_like(temp),\n",
    "                       output_hidden_states = True, output_attentions = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "metadata": {},
     "execution_count": 61
    }
   ],
   "source": []
  }
 ]
}