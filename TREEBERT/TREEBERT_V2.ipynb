{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.10 64-bit ('Pytorch': conda)"
  },
  "interpreter": {
   "hash": "b3174a1ca514c82740fdb711c545ebcc890d580e90aa5ede5ddc86ad6655ae32"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import random\n",
    "from copy import deepcopy as cc\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import gc\n",
    "from torch import nn\n",
    "from torch.nn import GELU as GELU\n",
    "import argparse\n",
    "from dataset import *\n",
    "gc.disable()\n",
    "gc.collect()\n",
    "import pdb"
   ]
  },
  {
   "source": [
    "- $ a = \\begin{matrix}  0 & 1 & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 1 \\\\ 0 & 0 & 0 & 0 \\\\ \\end{matrix} $ &#8594\n",
    "$ b = \\begin{matrix}  1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 1 \\\\ \\end{matrix} $ &#8594 \n",
    "$ c = \\begin{matrix}  0 & 0 & 0 & 0 \\\\ 1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\\\ \\end{matrix} $ &#8594\n",
    "$ tri\\_matrix = \\begin{matrix}  1 & 1 & 1 & 1 \\\\ 0 & 1 & 1 & 1 \\\\ 0 & 0 & 1 & 1 \\\\ 0 & 0 & 0 & 1 \\\\ \\end{matrix} $\n",
    "$ (a + c) = \\begin{matrix} 0 & 1 & 0 & 0 \\\\ 1 & 0 & 1 & 0 \\\\ 0 & 1 & 0 & 1 \\\\ 0 & 0 & 1 & 0 \\\\ \\end{matrix} $\n",
    "\n",
    "- $ SCORE =  \\begin{matrix}  \n",
    "\\langle Q_0, K_0 \\rangle & \\dots & \\langle Q_0, K_3 \\rangle\\\\ \\vdots & \\ddots & \\\\ \\langle Q_3, K_0 \\rangle &  & \\langle Q_3, K_3 \\rangle \\\\ \n",
    "\\end{matrix}\n",
    "= \\begin{matrix} S_{00} & \\dots & S_{03} \\\\ \\vdots & \\ddots & \\\\S_{30} & & S_{33} \\\\ \\end{matrix} $\n",
    "\n",
    "- $ SCORE  \\xrightarrow[- \\infty]{MASK(a+c)} \n",
    "\\begin{matrix} \n",
    "-\\infty & S_{01} & -\\infty & -\\infty \\\\ S_{10} & -\\infty & S_{12} & -\\infty \\\\ -\\infty & S_{21} & -\\infty & S_{23} \\\\ -\\infty & -\\infty & S_{32} & -\\infty \\\\ \\end{matrix} \n",
    "\\xrightarrow[]{SOFTMAAX(dim = 0)} \n",
    "\\begin{matrix} \n",
    "0 & P_{01} & 0 & 0 \\\\ P_{10} & 0 & P_{12} & 0 \\\\ 0 & P_{21} & 0 & P_{23} \\\\ 0 & 0 & P_{32} & 0 \\\\ \\end{matrix}  \n",
    "\\xrightarrow[]{Score^T\\dot (Score+\\epsilon)} \n",
    "\\begin{matrix} \n",
    "0 & \\sqrt{\\langle P_{01}, P_{01} +\\epsilon \\rangle} & 0 & 0 \\\\ \n",
    "\\sqrt{\\langle P_{10},P_{01} +\\epsilon \\rangle} & 0 & \\sqrt{\\langle P_{12},P_{21} +\\epsilon \\rangle} & 0 \\\\ \n",
    "0 & \\sqrt{\\langle P_{21},P_{12} +\\epsilon \\rangle }& 0 & \\sqrt{\\langle P_{23},P_{32} +\\epsilon \\rangle} \\\\ \n",
    "0 & 0 & \\sqrt{\\langle P_{32},P_{23} +\\epsilon \\rangle} & 0 \\\\ \\end{matrix}$\n",
    "\n",
    "- $\\hat{A^l} \\sim\n",
    "\\begin{matrix} \n",
    "0 & \\sqrt{\\langle P_{01}, P_{01}\\rangle} & 0 & 0 \\\\ \n",
    "\\sqrt{\\langle P_{10}P_{01} \\rangle} & 0 & \\sqrt{\\langle P_{12}P_{21} \\rangle} & 0 \\\\ \n",
    "0 & \\sqrt{\\langle P_{21}P_{12} \\rangle }& 0 & \\sqrt{\\langle P_{23}P_{32} \\rangle} \\\\ \n",
    "0 & 0 & \\sqrt{\\langle P_{32}P_{23} \\rangle}& 0 \\\\ \\end{matrix}\n",
    "\\equiv\n",
    "\\begin{matrix} \n",
    "0 & \\hat{a_0} & 0 & 0  \\\\ \n",
    "\\hat{a_0} & 0 & \\hat{a_1} & 0 \\\\ \n",
    "0 & \\hat{a_1} & 0 & \\hat{a_2} \\\\ \n",
    "0 & 0 & \\hat{a_2} & 0 \\\\ \\end{matrix}$\n",
    "\n",
    "- $A^l = A^{l-1} + (1 - A^{l-1})\\hat{A^l} = \n",
    "\\begin{matrix} 0 & a_0 & 0 & 0  \\\\ a_0 & 0 & a_1 & 0 \\\\ 0 & a_1 & 0 & a_2 \\\\ 0 & 0 & a_2 & 0 \\\\ \\end{matrix}\n",
    "\\xrightarrow[0]{MASK(a)} \n",
    "\\begin{matrix} 0 & log(a_0 + \\epsilon) & 0 & 0  \\\\ 0 & 0 & log(a_1 + \\epsilon) & 0 \\\\ 0 & 0 & 0 & log(a_2 + \\epsilon) \\\\ 0 & 0 & 0 & 0 \\\\ \\end{matrix}\n",
    "\\xrightarrow[]{C.tri\\_matrix}\n",
    "\\begin{matrix} \n",
    "0 & log(a_0 + \\epsilon) & log(a_0+ \\epsilon) & log(a_0+ \\epsilon)  \\\\ \n",
    "0 & 0 & log(a_1+ \\epsilon) & log(a_1+ \\epsilon) \\\\ \n",
    "0 & 0 & 0 & log(a_2 + \\epsilon) \\\\ \n",
    "0 & 0 & 0 & 0 \\\\ \\end{matrix}\n",
    "\\xrightarrow[tri\\_matrix^T.C]{log()} \\sim\n",
    "\\begin{matrix} \n",
    "0 &  log(a_0 + \\epsilon) & log(a_0)+log(a_1) & log(a_0)+log(a_1)+log(a_2)  \\\\ \n",
    "0 & 0 & log(a_1) & log(a_1)+log(a_2) \\\\ \n",
    "0 & 0 & 0 & log(a_2) \\\\ \n",
    "0 & 0 & 0 & 0 \\\\ \n",
    "\\end{matrix}$\n",
    "\n",
    "- $ \\equiv \n",
    "\\begin{matrix} 0 & log(a_0) & log(a_0a_1) & log(a_0a_1a_2)  \\\\ \n",
    "0 & 0 & log(a_1) & log(a_1a_2) \\\\ \n",
    "0 & 0 & 0 & log(a_2) \\\\ \n",
    "0 & 0 & 0 & 0 \\\\ \n",
    "\\end{matrix}\n",
    "\\xrightarrow[]{exp()}\n",
    "\\begin{matrix} 0 & C_{01} & C_{02} & C_{02}  \\\\ 0 & 0 & C_{12} & C_{13} \\\\ 0 & 0 & 0 & C_{23} \\\\ 0 & 0 & 0 & 0 \\\\ \\end{matrix}\n",
    "\\xrightarrow[]{ + C^T}\n",
    "\\begin{matrix} 0 & C_{01} & C_{02} & C_{02}  \\\\ \n",
    "C_{01} & 0 & C_{12} & C_{13} \\\\ \n",
    "C_{02} & C_{12} & 0 & C_{23} \\\\ \n",
    "C_{03} & C_{13} & C_{23} & 0 \\\\ \n",
    "\\end{matrix}$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config():\n",
    "    \"Configuration for BERT model\"\n",
    "    vocab_size: int = 30522             # Size of Vocabulary\n",
    "    dim: int = 768                      # Dimension of Hidden Layer in Transformer Encoder\n",
    "    n_layers: int = 12                  # Numher of Hidden Layers\n",
    "    n_heads: int = 12                   # Numher of Heads in Multi-Headed Attention Layers\n",
    "    dim_ff: int = 768*4                 # Dimension of Intermediate Layers in Positionwise Feedforward Net\n",
    "    activ_fn: str = \"gelu\"              # Non-linear Activation Function Type in Hidden Layers\n",
    "    p_drop_hidden: float = 0.1          # Probability of Dropout of various Hidden Layers\n",
    "    p_drop_attn: float = 0.1            # Probability of Dropout of Attention Layers\n",
    "    max_len: int = 512                  # Maximum Length for Positional Embeddings\n",
    "    n_segments: int = 2                 # Number of Sentence Segments\n",
    "    layer_norm_eps: int = 1e-12         # eps value for the LayerNorms\n",
    "    output_attentions : bool = False    # Weather to output the attention scores\n",
    "\n",
    "config = Config()\n",
    "INFINITY = 1e15\n",
    "EPSILON = 1e-15\n"
   ]
  },
  {
   "source": [
    "+ #  <u>Embedding Layer</u>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertEmbeddings(nn.Module):\n",
    "    \"The embedding module from word, position and token_type embeddings.\"\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.word_embeddings = nn.Embedding(cfg.vocab_size, cfg.dim, padding_idx=0) # token embedding\n",
    "        self.position_embeddings = nn.Embedding(cfg.max_len, cfg.dim) # position embedding\n",
    "        self.token_type_embeddings = nn.Embedding(cfg.n_segments, cfg.dim) # segment(token type) embedding\n",
    "\n",
    "        self.LayerNorm = nn.LayerNorm(cfg.dim, eps=cfg.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(cfg.p_drop_hidden)\n",
    "\n",
    "    def forward(self, x, seg):\n",
    "        # X -> B x S\n",
    "        # seg -> B x S\n",
    "\n",
    "        S = x.size(1)\n",
    "        pos = torch.arange(S, dtype=torch.long, device=x.device)\n",
    "        pos = pos[None, :].expand_as(x) # (S,) -> (B, S)\n",
    "\n",
    "        e = self.word_embeddings(x) + self.position_embeddings(pos) + self.token_type_embeddings(seg)\n",
    "        e = self.LayerNorm(e)\n",
    "        e = self.dropout(e)\n",
    "        return e"
   ]
  },
  {
   "source": [
    "* # <u>Self Attention Layer</u>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupAttention2(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super(GroupAttention2, self).__init__()\n",
    "        self.d_model = cfg.dim\n",
    "        self.linear_key = nn.Linear(cfg.dim, cfg.dim)\n",
    "        self.linear_query = nn.Linear(cfg.dim, cfg.dim)\n",
    "        self.LayerNorm = nn.LayerNorm(cfg.dim, eps= cfg.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(cfg.p_drop_attn)\n",
    "\n",
    "        torch.nn.init.xavier_normal_(self.linear_key.weight)\n",
    "        torch.nn.init.zeros_(self.linear_key.bias)\n",
    "\n",
    "        torch.nn.init.xavier_normal_(self.linear_query.weight)\n",
    "        torch.nn.init.zeros_(self.linear_query.bias)\n",
    "\n",
    "        torch.nn.init.uniform_(self.LayerNorm.weight)\n",
    "        torch.nn.init.zeros_(self.LayerNorm.bias)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask, ip_event_loc, ip_event_mask, prior_A):\n",
    "        ''' \n",
    "        hidden_states = B x S x D\n",
    "        attention_mask = B x S(V/K)\n",
    "        A_prior = B x S(Q) x S(V/K)\n",
    "        '''\n",
    "\n",
    "        B, S_full = hidden_states.size()[:2]\n",
    "        hidden_event_subset = hidden_states[np.arange(B)[:, None], ip_event_loc]\n",
    "        context = self.LayerNorm(hidden_event_subset)\n",
    "        B, S = hidden_event_subset.size()[:2]\n",
    "\n",
    "        a = torch.from_numpy(np.diag(np.ones(S - 1, dtype=np.int32),1)).to(hidden_states.device)\n",
    "        b = torch.from_numpy(np.diag(np.ones(S, dtype=np.int32),0)).to(hidden_states.device)\n",
    "        c = torch.from_numpy(np.diag(np.ones(S - 1, dtype=np.int32),-1)).to(hidden_states.device)\n",
    "        tri_matrix = torch.from_numpy(np.triu(np.ones([S, S], dtype=np.float32),0)).to(hidden_states.device)\n",
    "\n",
    "        a_full = torch.from_numpy(np.diag(np.ones(S_full - 1, dtype=np.int32),1)).to(hidden_states.device)\n",
    "        b_full = torch.from_numpy(np.diag(np.ones(S_full, dtype=np.int32),0)).to(hidden_states.device)\n",
    "        c_full = torch.from_numpy(np.diag(np.ones(S_full - 1, dtype=np.int32),-1)).to(hidden_states.device)\n",
    "        tri_matrix_full = torch.from_numpy(np.triu(np.ones([S_full, S_full], dtype=np.float32),0)).to(hidden_states.device)\n",
    "\n",
    "        # mask -> BxSxS\n",
    "        mask = ip_event_mask[:, None, :] & (a+c)[None, :, :]\n",
    "        \n",
    "        key = self.linear_key(context)\n",
    "        query = self.linear_query(context)\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / (self.d_model / 2)\n",
    "        scores = scores.masked_fill(mask == 0, (-1 * INFINITY))\n",
    "\n",
    "        # Compute the A for the [et_sep] locations \n",
    "        # A -> B x S x S\n",
    "        A = F.softmax(scores, dim=-1)\n",
    "        A = torch.sqrt(A * A.transpose(-2,-1) + EPSILON)\n",
    "\n",
    "        # A -> B x (S-1) ; Extract the diagonal(offset 1) elements from the A\n",
    "        A = torch.cat([torch.diagonal(A[batch], 1)[None, :] for batch in range(B)])\n",
    "\n",
    "        # scatter the above A matrix in the A_full (B x S_full) matrix\n",
    "        A_new = cc(attention_mask).float()\n",
    "        A_new = A_new.scatter(1, ip_event_loc[:, :-1], A.float())\n",
    "\n",
    "        A_new = A_new[:, :-1]\n",
    "        A_new= [torch.diag(A_new[batch, :], 1)[None, :, :] for batch in range(B)]\n",
    "        A_new = torch.cat(A_new)\n",
    "        A_new += A_new.transpose(-1, -2).contiguous()\n",
    "        A_new = prior_A + (1. - prior_A)*A_new\n",
    "\n",
    "        t = torch.log(A_new + EPSILON).masked_fill(a_full==0, 0).matmul(tri_matrix_full)\n",
    "        C_prior = tri_matrix_full.matmul(t).exp().masked_fill((tri_matrix_full.int()-b_full)==0, 0)\n",
    "        C_prior = C_prior + C_prior.transpose(-2, -1).contiguous() + torch.from_numpy(np.diag(np.ones(S_full)))\n",
    "        \n",
    "        return C_prior.float(), A_new.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupAttention(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super(GroupAttention, self).__init__()\n",
    "        self.d_model = cfg.dim\n",
    "        self.linear_key = nn.Linear(cfg.dim, cfg.dim)\n",
    "        self.linear_query = nn.Linear(cfg.dim, cfg.dim)\n",
    "        self.LayerNorm = nn.LayerNorm(cfg.dim, eps= cfg.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(cfg.p_drop_attn)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask, prior_A):\n",
    "        ''' \n",
    "        dim( hidden_states ) = B x S x D\n",
    "        dim( attention_mask ) = B x S(V/K)\n",
    "        dim( prior ) = S(Q) x S(V/K)\n",
    "        '''\n",
    "        B, S = hidden_states.size()[:2]\n",
    "\n",
    "        context = self.LayerNorm(hidden_states)\n",
    "        a = torch.from_numpy(np.diag(np.ones(S - 1, dtype=np.int32),1))\n",
    "        b = torch.from_numpy(np.diag(np.ones(S, dtype=np.int32),0))\n",
    "        c = torch.from_numpy(np.diag(np.ones(S - 1, dtype=np.int32),-1))\n",
    "        tri_matrix = torch.from_numpy(np.triu(np.ones([S, S], dtype=np.float32),0))\n",
    "\n",
    "        #mask = eos_mask & (a+c) | b\n",
    "        mask = attention_mask[:, None, :] & (a+c)[None, :, :]\n",
    "        \n",
    "        key = self.linear_key(context)\n",
    "        query = self.linear_query(context)\n",
    "        \n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / (self.d_model / 2)\n",
    "        \n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        A = F.softmax(scores, dim=-1)\n",
    "        A = torch.sqrt(A * A.transpose(-2,-1) + 1e-9)\n",
    "        A = prior_A + (1. - prior_A)*A\n",
    "\n",
    "        t = torch.log(A + 1e-9).masked_fill(a==0, 0).matmul(tri_matrix)\n",
    "        C_prior = tri_matrix.matmul(t).exp().masked_fill((tri_matrix.int()-b)==0, 0)    \n",
    "        C_prior = C_prior + C_prior.transpose(-2, -1) + torch.from_numpy(np.diag(np.zeros(S)))\n",
    "        \n",
    "        return C_prior.float(), A.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# X -> self_attn -> X [calculated by self attention]\n",
    "class BertSelfAttention(nn.Module):\n",
    "    \"\"\" Multi-Headed Dot Product Attention \"\"\"\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.query = nn.Linear(cfg.dim, cfg.dim)\n",
    "        self.key = nn.Linear(cfg.dim, cfg.dim)\n",
    "        self.value = nn.Linear(cfg.dim, cfg.dim)\n",
    "        self.dropout = nn.Dropout(cfg.p_drop_attn)\n",
    "        self.n_heads = cfg.n_heads\n",
    "\n",
    "    def forward(self, x, attention_mask = None, output_attentions = False, C_prior = None):\n",
    "        \"\"\"\n",
    "        x, q(query), k(key), v(value) : (B(batch_size), S(seq_len), D(dim))\n",
    "        mask : (B(batch_size) x S(seq_len))\n",
    "        * split D(dim) into (H(n_heads), W(width of head)) ; D = H * W\n",
    "        attention_mask_dim = B x S (dim S is for every value in the \"x\"), so need to repeat it for every query\n",
    "        \"\"\"\n",
    "        B, S, D = x.shape\n",
    "        H = self.n_heads\n",
    "        W = int( D/H )\n",
    "        assert W * H == D\n",
    "\n",
    "        # (B, S, D) -proj-> (B, S, D) -split-> (B, S, H, W) -trans-> (B, H, S, W)\n",
    "        q, k, v = self.query(x), self.key(x), self.value(x)\n",
    "        q, k, v = q.reshape((B, S, H, W)), k.reshape((B, S, H, W)), v.reshape((B, S, H, W))\n",
    "        q, k, v = q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2)\n",
    "\n",
    "        # (B, H, S(Q), W) @ (B, H, W, S(K/V)) -> (B, H, S(Q), S(K/V)) -Masking -> softmax-> (B, H, S(Q), S(K/V))\n",
    "        attn_scores = q @ k.transpose(-2, -1) / np.sqrt(k.size(-1))\n",
    "        \n",
    "        # Set the attention score at places where MASK = 0 to very low value (-1e9)\n",
    "        if attention_mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(attention_mask[:, None, None, :] == 0, (-1 * INFINITY))\n",
    "        attn_scores = self.dropout(F.softmax(attn_scores, dim=-1))\n",
    "    \n",
    "        # C_prior -> B x S x S\n",
    "        if C_prior != None:\n",
    "            attn_scores = attn_scores * C_prior[:, None, :, :]\n",
    "\n",
    "        # (B, H, S, S) @ (B, H, S, W) -> (B, H, S, W) -trans-> (B, S, H, W) -merge-> (B, S, D)\n",
    "        hidden_states = (attn_scores @ v).transpose(1, 2).contiguous()\n",
    "        hidden_states = hidden_states.reshape(B, S, D)\n",
    "\n",
    "        result = {}\n",
    "        result['hidden_states'] = hidden_states \n",
    "        result['attn_scores'] =  None\n",
    "        if output_attentions :\n",
    "            result['attn_scores'] =  attn_scores\n",
    "\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  f(X) -> Linear (D => D) -> Dropout -> X1\n",
    "#  X, f(X) -> LayerNorm( X + X1 )\n",
    "class BertSelfOutput(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(cfg.dim, cfg.dim)\n",
    "        self.LayerNorm = nn.LayerNorm(cfg.dim, eps= cfg.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(cfg.p_drop_hidden)\n",
    "\n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X -> self_attn -> f(X) -> Linear (D => D) -> Dropout -> X1\n",
    "# Y = LayerNorm( X + X1 )\n",
    "class BertAttention(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.self = BertSelfAttention(cfg)\n",
    "        self.output = BertSelfOutput(cfg)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None, output_attentions = False, C_prior = None):\n",
    "        self_outputs = self.self(hidden_states,  attention_mask, output_attentions, C_prior)\n",
    "        attention_output = self.output(self_outputs['hidden_states'], hidden_states)\n",
    "\n",
    "        outputs = {'hidden_states': attention_output, 'attn_scores': self_outputs['attn_scores']}\n",
    "        return outputs        "
   ]
  },
  {
   "source": [
    "* # <u>Forward Expansion layer</u>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f(X) -> Linear(D => 4xD) -> GELU() -> X2\n",
    "class BertIntermediate(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(cfg.dim, cfg.dim_ff)\n",
    "        self.GELU = nn.GELU()\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.GELU(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "# f(X) -> Linear(4xD => D) -> Drouput -> X1\n",
    "#  X, f(X) -> LayerNorm( X + X1 )\n",
    "class BertOutput(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(cfg.dim_ff, cfg.dim)\n",
    "        self.LayerNorm = nn.LayerNorm(cfg.dim, eps=cfg.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(cfg.p_drop_hidden)\n",
    "\n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "        return hidden_states"
   ]
  },
  {
   "source": [
    "* # <u>Defining single BERT-encoder layer</u>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertLayer(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.group_attention = GroupAttention2(cfg)\n",
    "        self.attention = BertAttention(cfg)\n",
    "        self.intermediate = BertIntermediate(cfg)\n",
    "        self.output = BertOutput(cfg)\n",
    "\n",
    "    def forward(self, hidden_states, ip_event_loc, ip_event_mask, attention_mask=None, output_attentions = False, A_prior = None):\n",
    "        # pdb.set_trace()\n",
    "        C_prior = None\n",
    "\n",
    "        if A_prior != None:\n",
    "            # C_prior, A_prior = self.group_attention(hidden_states, attention_mask, A_prior)\n",
    "            C_prior, A_prior = self.group_attention(hidden_states = hidden_states, ip_event_loc = ip_event_loc, \n",
    "                                                    attention_mask = attention_mask, ip_event_mask = ip_event_mask, prior_A = A_prior)\n",
    "\n",
    "        self_attention_outputs = self.attention(hidden_states, attention_mask, output_attentions, C_prior)\n",
    "        intermediate_output = self.intermediate(self_attention_outputs['hidden_states'])\n",
    "        layer_output = self.output(intermediate_output, self_attention_outputs['hidden_states'])\n",
    "\n",
    "        outputs = {'hidden_states':layer_output, 'attn_scores': self_attention_outputs['attn_scores'], \n",
    "                  'A_prior': A_prior, 'C_prior': C_prior}\n",
    "        return outputs"
   ]
  },
  {
   "source": [
    "* # <u>Defining the BERT model</u>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertEncoder(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layer = nn.ModuleList([BertLayer(cfg) for _ in range(cfg.n_layers)])\n",
    "\n",
    "    def forward(self, hidden_states, ip_event_loc, ip_event_mask, attention_mask=None, output_attentions = False, \n",
    "                output_hidden_states = False, A_prior = None, output_A_prior = False, output_C_prior = False):   \n",
    "\n",
    "        all_hidden_states = [] if output_hidden_states else None\n",
    "        all_self_attentions = [] if output_attentions else None\n",
    "        all_A_priors = [] if output_A_prior else None\n",
    "        all_C_priors = [] if output_C_prior else None\n",
    "\n",
    "        for i, layer_module in enumerate(self.layer):\n",
    "            if output_hidden_states:\n",
    "                all_hidden_states.append(hidden_states)\n",
    "\n",
    "            layer_outputs = layer_module(hidden_states, ip_event_loc, ip_event_mask, attention_mask, output_attentions, A_prior)\n",
    "            hidden_states, attn_scores = layer_outputs['hidden_states'], layer_outputs['attn_scores'], \n",
    "            C_prior, A_prior = layer_outputs['C_prior'], layer_outputs['A_prior']\n",
    "\n",
    "            if output_attentions:\n",
    "                all_self_attentions.append(attn_scores)\n",
    "            if output_A_prior:\n",
    "                all_A_priors.append(A_prior)\n",
    "            if output_C_prior:\n",
    "                all_C_priors.append(C_prior)\n",
    "\n",
    "        outputs = {'hidden_states': hidden_states, 'attn_scores': all_self_attentions, \n",
    "                  'all_hidden_states': all_hidden_states, 'all_A_priors': all_A_priors, 'all_C_priors': all_C_priors}\n",
    "        return outputs \n",
    "\n",
    "\n",
    "class my_BERT(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.embeddings = BertEmbeddings(cfg)\n",
    "        self.encoder = BertEncoder(cfg)\n",
    "    \n",
    "    def forward(self, x, seg, ip_event_loc, ip_event_mask, attention_mask=None, output_attentions = False, \n",
    "                output_hidden_states = False, A_prior = None, output_A_prior = False, output_C_prior = False):\n",
    "\n",
    "        embedding_output = self.embeddings(x, seg)\n",
    "        encoded_output = self.encoder(hidden_states = embedding_output, \n",
    "                                      attention_mask = attention_mask, \n",
    "                                      ip_event_loc = ip_event_loc, \n",
    "                                      ip_event_mask = ip_event_mask,\n",
    "                                      output_attentions = output_attentions, \n",
    "                                      output_hidden_states = output_hidden_states,\n",
    "                                      A_prior = A_prior,\n",
    "                                      output_A_prior = output_A_prior,\n",
    "                                      output_C_prior = output_C_prior)\n",
    "\n",
    "        return encoded_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()\n",
    "MY_BERT = my_BERT(config)\n",
    "BERT = BertModel.from_pretrained('bert-base-uncased')\n",
    "MY_BERT.load_state_dict(BERT.state_dict(), strict = False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Parametes in the model that are not in BERT:: {'pooler.dense.bias', 'pooler.dense.weight'}\n"
     ]
    }
   ],
   "source": [
    "MY_BERT_params = set([n for n, _ in MY_BERT.named_parameters()])\n",
    "BERT_params = set([n for n, _ in BERT.named_parameters()])\n",
    "print(f'Parametes in the model that are not in BERT:: {BERT_params.difference(MY_BERT_params)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tree2bert_dataset(data_path = './../Data/raw/seq2seq/train_short_prefix.txt.val',\n",
    "                            max_seq_len = 256,\n",
    "                            max_ev_len = 20)\n",
    "\n",
    "fast_tokenizer = BertWordPieceTokenizer(vocab='./vocab/vocab.txt')\n",
    "fast_tokenizer.add_special_tokens(['[et_sep]', '[ea_sep]', '[ds_sep]'])\n",
    "fast_tokenizer.enable_truncation(max_length = 256)\n",
    "fast_tokenizer.enable_padding(length=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emb_layer = BertEmbeddings(config)\n",
    "# att_layer = BertSelfAttention(config)\n",
    "# grp_att_layer = GroupAttention2(config)\n",
    "\n",
    "ip_text_tok = dataset[:2]['text_tok_src'].long()\n",
    "ip_text_mask = dataset[:2]['text_mask_src'].long()\n",
    "ip_event_loc = dataset[:2]['event_loc_src'].long()\n",
    "ip_event_mask = dataset[:2]['event_mask_src'].long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_initial_A(ip_text_mask, ip_event_loc):\n",
    "\n",
    "    A_initial = cc(ip_text_mask)*1.0\n",
    "\n",
    "    # Set all the locations with [et_sep] tokens as 0.0\n",
    "    A_initial = A_initial.scatter(1, ip_event_loc.long(), torch.zeros_like(ip_event_loc).float())\n",
    "\n",
    "    # exclude the last token as this vector will be a diagonal with offset 1\n",
    "    A_initial = A_initial[:, :-1]\n",
    "\n",
    "    # Create a diagonal matrix with this A as the diagonal at offset 1\n",
    "    A_initial= [torch.diag(A_initial[i, :].float(), 1)[None, :, :] for i in range(A_initial.shape[0])]\n",
    "    A_initial = torch.cat(A_initial)\n",
    "\n",
    "    # Add it's transpose, as the A matrix is supposed to be symmetric\n",
    "    A_initial += A_initial.transpose(-1, -2).contiguous()\n",
    "    return A_initial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_initial = get_initial_A(ip_text_mask, ip_event_loc)\n",
    "result = MY_BERT(x = ip_text_tok, \n",
    "                seg = torch.ones_like(ip_text_tok), \n",
    "                ip_event_loc = ip_event_loc, \n",
    "                ip_event_mask = ip_event_mask, \n",
    "                attention_mask = ip_text_mask, \n",
    "                output_attentions = False, \n",
    "                output_hidden_states = False, \n",
    "                A_prior = A_initial, \n",
    "                output_A_prior = False,\n",
    "                output_C_prior = True\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 0.7064, 0.7064, 0.7064, 0.7064, 0.7064, 0.7064, 0.7064,\n",
       "        0.7064, 0.7064, 0.7064, 0.7064, 0.7064, 0.7064, 0.7064, 0.7064, 0.7064,\n",
       "        0.7064, 0.7064, 0.7064, 0.7064, 0.3498, 0.3498, 0.3498, 0.3498, 0.3498,\n",
       "        0.3498, 0.3498, 0.3498, 0.3498, 0.1767, 0.1767, 0.1767, 0.1767, 0.1767,\n",
       "        0.1767, 0.1767, 0.1767, 0.1767, 0.1767, 0.1767, 0.1767, 0.1767, 0.1767,\n",
       "        0.1767, 0.1767, 0.1767, 0.1767, 0.1767, 0.1767],\n",
       "       grad_fn=<SliceBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 205
    }
   ],
   "source": [
    "class TreeBERT2BERT(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(TreeBERT2BERT, self).__init__()\n",
    "        self.config = config\n",
    "        self.encoder = Tree_BERT(self.config)\n",
    "\n",
    "        BERT = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.encoder.load_state_dict(BERT.state_dict(), strict = False);\n",
    "\n",
    "        del BERT\n",
    "        self.decoder = EncoderDecoderModel.from_encoder_decoder_pretrained('bert-base-uncased', 'bert-base-uncased').decoder\n",
    "    \n",
    "    def forward(self, src_text_tok, src_text_mask, src_event_loc, src_event_pad_mask, tar_text_tok, tar_pad_mask):\n",
    "\n",
    "        # indices with label -100 will be ignored in the loss computation\n",
    "        decoder_label = cc(tar_text_tok)\n",
    "        decoder_label[tar_text_tok == 0] = -100\n",
    "\n",
    "        # Get the encoded input\n",
    "        A_initial = self.get_initial_A(self, ip_text_mask = src_text_mask, ip_event_loc = src_event_loc)\n",
    "        encoded_src = self.encoder(x = src_text_tok, \n",
    "                                   seg = torch.ones_like(src_text_tok), \n",
    "                                   ip_event_loc = src_event_loc, \n",
    "                                   ip_event_mask = src_event_pad_mask, \n",
    "                                   attention_mask = src_text_mask, \n",
    "                                   output_attentions = False, \n",
    "                                   output_hidden_states = False, \n",
    "                                   A_prior = A_initial, \n",
    "                                   output_A_prior = False,\n",
    "                                   output_C_prior = True)\n",
    "        \n",
    "        # Pass the encoded input to the decoder\n",
    "        decoder_out = self.decoder(input_ids = tar_text_tok,\n",
    "                            attention_mask = tar_pad_mask,\n",
    "                            encoder_hidden_states = encoded_src['hidden_states'],\n",
    "                            encoder_attention_mask = src_text_mask,\n",
    "                            labels = decoder_label)\n",
    "        return  decoder_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([ 10,  30,  39,  59,  68,  94, 104, 106, 107, 108, 109, 110, 111, 112,\n",
       "        113, 114, 115, 116, 117, 118])"
      ]
     },
     "metadata": {},
     "execution_count": 204
    }
   ],
   "source": [
    "ip_event_loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}